{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import csv\n",
    "import re\n",
    "from pyhanlp import *\n",
    "import pkuseg\n",
    "from stanfordcorenlp import StanfordCoreNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取csv至字典\n",
    "csvFile = open(\"test.csv\", encoding='utf-8',)\n",
    "reader = csv.reader(csvFile)\n",
    "\n",
    "# 建立空字典\n",
    "senlst = []\n",
    "for item in reader:\n",
    "    # 忽略第一行\n",
    "    if reader.line_num == 1:\n",
    "        continue\n",
    "    senlst.append(item[1])\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(raw_sen):\n",
    "    count = 0\n",
    "    tmp_list = []\n",
    "    for ele in raw_sen.strip().split(' '):\n",
    "        _tmp_list = []\n",
    "        for _ in range(len(ele)):\n",
    "            _tmp_list.append(count)\n",
    "            count += 1\n",
    "        tmp_list.append(str(_tmp_list).replace(' ', ''))\n",
    "\n",
    "    return ' '.join(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\szh\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.001 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#jieba分词\n",
    "jieba.load_userdict('C:/Users/szh/Desktop/szh/Python_test/NLP/Word_Segmentation/dict.txt') \n",
    "result=[]\n",
    "n=1\n",
    "for item in senlst:\n",
    "    result.append([str(n),transfer(' '.join(jieba.cut(item)))])\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "另外 旅游 、 侨汇 也 是 经济 收入 的 重要 组成 部分 ， 制造业 规模 相对 较小 。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "re.sub('\\/[a-zA-Z0-9]*,|\\/[a-z]*|\\[|\\]','',str(HanLP.segment(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HanLP\n",
    "result=[]\n",
    "n=1\n",
    "for item in senlst:\n",
    "    result.append([str(n),transfer(re.sub('\\/[a-zA-Z0-9]*,|\\/[a-z]*|\\[|\\]','',str(HanLP.segment(item))))])\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkuseg分词\n",
    "pku_seg = pkuseg.pkuseg()\n",
    "result=[]\n",
    "n=1\n",
    "for item in senlst:\n",
    "    result.append([str(n),transfer(' '.join(pku_seg.cut(item)))])\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "而 计算机 翻译 系统 与 现代 汉语 分析 相辅相成 ， 正 推动 着 人工 智能 科学 的 前进 … …\n"
     ]
    }
   ],
   "source": [
    "#stanfordcorenlp\n",
    "stanford_nlp = StanfordCoreNLP('D:/Anaconda/envs/szh/Lib/site-packages/stanfordcorenlp/stanford-corenlp-full-2018-02-27', lang='zh')\n",
    "print(' '.join(stanford_nlp.word_tokenize('而计算机翻译系统与现代汉语分析相辅相成，正推动着人工智能科学的前进……')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stanfordcorenlp\n",
    "stanford_nlp = StanfordCoreNLP('D:/Anaconda/envs/szh/Lib/site-packages/stanfordcorenlp/stanford-corenlp-full-2018-02-27', lang='zh')\n",
    "result=[]\n",
    "n=1\n",
    "for item in senlst:\n",
    "    result.append([str(n),transfer(' '.join(stanford_nlp.word_tokenize(item)))])\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101', '[0,1] [2,3] [4,5] [6] [7] [8,9] [10,11] [12] [13] [14] [15,16] [17]']\n"
     ]
    }
   ],
   "source": [
    "print(result[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileHeader = [\"id\", \"expected\"]\n",
    "csvFile = open(\"result5.csv\", \"w\",newline='')\n",
    "writer = csv.writer(csvFile)\n",
    "writer.writerow(fileHeader)\n",
    "for item in result:\n",
    "    writer.writerow(item)\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc27602090462304dd3f86372efd19e69b7030eadfbaca8857bfdb0924f069a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('szh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
